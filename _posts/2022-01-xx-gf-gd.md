---
layout:     post
title:      Does Gradient Flow Over Neural Networks Really Represent Gradient Descent?
date:       2022-1-xx xx:00:00
author:     Nadav Cohen
visible:    False
---

A lot has been said in this blog (cf. [post by Sanjeev](http://www.offconvex.org/2019/06/03/trajectories/)) about the importance of studying trajectories of gradient descent (***GD***) for understanding deep learning.
Researchers often conduct such studies by considering *gradient flow* (***GF***), equivalent to GD with infinitesimally small step size.
Much was learned from analyzing GF over neural networks (***NNs***), but to what extent *do these learnings apply to GD with practical step size?*
This is an open question in deep learning theory.
[Omer Elkabetz](https://www.linkedin.com/in/omer-elkabetz/) and I investigated it in a recent NeurIPS 2021 spotlight [paper](https://openreview.net/forum?id=iX0TSH45eOd).
In a nutshell, we found that, although in general an exponentially small step size is required for guaranteeing that GD is well represented by GF, specifically over NNs, a much larger step size can suffice.
This allows ***immediate translation of analyses for GF over NNs to results for GD***.
The translation bears potential to shed light on both optimization and generalization (implicit regularization) in deep learning, and indeed, we exemplify its use for proving what is, to our knowledge, the ***first guarantee of random near-zero initialization almost surely leading GD over a deep (three or more layer) NN of fixed size to efficiently converge to global minimum***.
The remainder of this post provides more details; for the full story see our [paper](https://openreview.net/forum?id=iX0TSH45eOd).

## GF: a continuous surrogate for GD

Let $f : \mathbb{R}^d \to \mathbb{R}$ be an objective function (e.g. training loss of deep NN) that we would like to minimize via GD with step size $\eta > 0$: 
\[
\boldsymbol\theta_{k + 1} = \boldsymbol\theta_k - \eta \nabla f ( \boldsymbol\theta_k ) ~ ~ ~ \text{for} ~ k = 0 , 1 , 2 , ...
\qquad \color{purple}{\text{(GD)}}
\]
We may imagine a continuous curve $\boldsymbol\theta : \[ 0 , \infty ) \to \mathbb{R}^d$ that passes through the $k$'th GD iterate at time $t = k \eta$.
This would imply $\boldsymbol\theta ( t + \eta ) = \boldsymbol\theta ( t ) - \eta \nabla f ( \boldsymbol\theta ( t ) )$, which we can write as $\frac{1}{\eta} ( \boldsymbol\theta ( t + \eta ) - \boldsymbol\theta ( t ) ) = - \nabla f ( \boldsymbol\theta ( t ) )$.
In the limit of infinitesimally small step size ($\eta \to 0$), we obtain the following characterization for $\boldsymbol\theta ( \cdot )$:
\[
\frac{d}{dt} \boldsymbol\theta ( t ) = - \nabla f ( \boldsymbol\theta ( t ) ) ~ ~ ~ \text{for} ~ t \geq 0 
.
\qquad \color{purple}{\text{(GF)}}
\]
This differential equation is known as GF, and represents a continuous surrogate for GD.

<div style="text-align:center;">
<img style="width:540px;" src="http://www.offconvex.org/assets/gf-gd-illustration.png" />
<br>
<i><b>Figure 1:</b> 
Illustration of GD and its continuous surrogate GF.
</i>
</div>
<br>

GF brings forth the possibility of employing a vast array of continuous mathematical machinery for studying GD.
It is for this reason that GF has become a popular model in deep learning theory (see our [paper](https://openreview.net/forum?id=iX0TSH45eOd) for a long list of works analyzing GF over NNs).
There is only one problem with this state of affairs $-$ GF assumes infinitesimal step size for GD!
To what extent does GF over NNs represent GD with positive step size?
This open question is the topic of our [work](https://openreview.net/forum?id=iX0TSH45eOd).

## GD as numerical integrator for GF

The question of proximity between GF and GD (with positive step size) is closely related to an area of numerical analysis known as *numerical integration*.
There, the motivation is the other way around $-$ given a differential equation $\frac{d}{dt} \boldsymbol\theta ( t ) = \boldsymbol g ( \boldsymbol\theta ( t ) )$ induced by some vector field $\boldsymbol g : \mathbb{R}^d \to \mathbb{R}^d$, the interest lies on a continuous solution $\boldsymbol\theta : \[ 0 , \infty ) \to \mathbb{R}^d$, and numerical integration algorithms are used for obtaining approximations.
A classic numerical integrator, known as *Euler's method*, is given by $\boldsymbol\theta_{k + 1} = \boldsymbol\theta_k + \eta \boldsymbol g ( \boldsymbol\theta_k )$ for $k = 0 , 1 , 2 , ...$, where $\eta > 0$ is a predetermined step size.
With Euler's method, the goal is for the $k$'th iterate to approximate the sought-after continuous solution at time $k \eta$, meaning $\boldsymbol\theta_k \approx \boldsymbol\theta ( k \eta )$.
At this point it should be evident that when the vector field $\boldsymbol g ( \cdot )$ is chosen to be minus the gradient of an objective function $f : \mathbb{R}^d \to \mathbb{R}$, i.e. $\boldsymbol g ( \cdot ) = - \nabla f ( \cdot )$, the given differential equation is no other than GF, and its numerical integration via Euler's method yields no other than GD!
We can therefore employ known results from numerical integration to bound the distance between GF and GD!

## GF matches GD if its trajectory is roughly convex

There exist classic results bounding the approximation error of Euler's method, but these are too coarse for our purposes.
Instead, we invoke a modern result known as "Fundamental Theorem" (cf. [Hairer et al. 1993](https://link.springer.com/book/10.1007/978-3-540-78862-1)), which implies the following:

> **Theorem 1:**
> The distance between the GD iterate $\boldsymbol\theta_k$ and the GF trajectory $\boldsymbol\theta ( \cdot )$ at time $t = k \eta$, is upper bounded as:
>
\[ 
\| \boldsymbol\theta_k - \boldsymbol\theta ( t ) \| \leq \mathcal{O} \Big( e^{- \smallint_0^{~ t} \lambda_{min} \left( \nabla^2 f ( \boldsymbol\theta ( t') ) \right) dt'} t \eta \Big)
, 
\]
> where $\lambda_{min} \left( \nabla^2 f ( \cdot ) \right)$ is the minimal eigenvalue of the Hessian $\nabla^2 f ( \cdot )$.

As expected, we may ensure that GD is arbitrarily close to GF by using a sufficiently small step size $\eta$.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Second, in order to guarantee eps approximation eta must be O(...), i.e. exponential in ... --- the integral of the min eigenval of hessian along GF trajectory
Over convex obj the min eigenval of hessian is non-negative throughout, and the bound tells us that eta need not be small for GD to be well aproximated by GF
On the other hand, over non-convex obj, a-priori the eigenval along GF trajectory could be very negative, and for the guarantee to hold eta needs to be exponentially small
We prove in our paper that there exist cases (with non-convex obj) where if eta is not exp small GD and GF will not be close: [claim]
However, not all hope is lost --- it could be that even though a given obj is non-convex, specifically over GF trajectories it is "roughly convex," i.e. the min eigenvals of Hessian is "not too negative"
If this is the case then a moderately small step size can suffice in order for the guarantee to hold

## Trajectories of GF over NNs are roughly convex

Being interested in the match between GF and GD over NNs, we analyzed the geometry of GF trajectories over training losses of NNs with homo activations (e.g. linear, ReLU, leaky ReLU)
We prove the following theorem: [theorem --- even though min eigenval of Hessian is arbitrarily negative in general, around GF trajectories init near zero is no less than slightly negative]
Combined with the outcome of the Fundamental Theorem (Eq X), this suggests that over NNs with homo activations, moderately small step size for GD suffices in order for it to closely follow GF
We verify this prospect empirically, demonstrating that in basic deep learning settings, reducing the step size for GD often leads to only slight changes in its trajectory --- see fig below [figure]

## Translating analyses of GF to results for GD

Theorems X and Y together form a tool for automatically translating analyses of GF over NNs to results for GD
This means that the continuous math machinery which GF enjoys can now be leveraged for analyzing the practical algorithm run in practice --- GD
Analyses of GF over NNs often entail trajectory characterizations that establish convergence to global min and/or provide info on the type of solutions found (cite)
Accordingly, the machinery we developed for translating the analyses to results for GD bear potential to shed light on both optimization and generalization (implicit regularization) in deep learning
To exemplify this point, we analyze GF over deep linear NN with 1D output, proving the following result: [proposition]
Its translation yields an analogous result for GD: [theorem]
To our knowledge, this is the first guarantee of random near-zero initialization almost surely leading GD over a deep (three or more layer) NN of fixed size to efficiently converge to global minimum!

## What about large step size, momentum, stochasticity?

Recent evidence suggests that when running GD over NNs, using large step size is often beneficial in terms of generalization
While the large step size benefit is not captured by standard GF, recent works argue that it is captured by certain modifications of GF
There have also been modifications proposed for capturing other features of contemporary DL optimization such as momentum and stochasticity
Extending our translation machinery to account for these modifications of GF is a promising direction for future work
We believe the vast bodies of lit on continuous dynamical systems, and gradient flow in particular, will pave way to unraveling mysteries behind deep learning.
